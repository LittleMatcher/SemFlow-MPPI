"""
Training Script for L2 Layer with Mock L3 Data

This script trains the L2 Safety-Embedded CFM layer using synthetic
cost maps and trajectories generated by MockL3Dataset.

Usage:
    python train_l2_mock.py --num_samples 5000 --epochs 50
    python train_l2_mock.py --data_source generated --data_dir traj_data/cfm_env_rrt --epochs 500 --batch_size 64 --lambda_jerk 0.01 --lambda_acc 0.01
    python train_l2_mock.py --style_mode safe --batch_size 16

If Train/Val Loss hardly drop (especially Acc >> Pos,Vel):
  - Data: generate with --max_acc 50 or clip_acc_npz so |acc| is bounded.
  - Loss: use --lambda_jerk 0.1 (default) so jerk does not dominate gradient.
  - Batches: use smaller --batch_size (e.g. 128) so more gradient updates per epoch.
"""

import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np
from tqdm import tqdm

from cfm_flowmp.data import MockL3Dataset, FlowMPEnvDataset, create_l2_dataloaders
from cfm_flowmp.models import create_l2_safety_cfm, L2Config
from cfm_flowmp.training import FlowMatchingLoss, FlowMatchingConfig, FlowInterpolator


def parse_args():
    parser = argparse.ArgumentParser(description="Train L2 layer with mock L3 data")
    
    # Data parameters
    parser.add_argument('--data_source', type=str, default='mock',
                        choices=['mock', 'generated'],
                        help='mock: MockL3Dataset; generated: load from .npz (see --data_dir)')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='Directory containing data.npz (required if data_source=generated)')
    parser.add_argument('--num_samples', type=int, default=5000,
                        help='Number of training samples (mock only)')
    parser.add_argument('--val_samples', type=int, default=500,
                        help='Number of validation samples (mock only)')
    parser.add_argument('--map_size', type=int, default=64,
                        help='Cost map size')
    parser.add_argument('--seq_len', type=int, default=64,
                        help='Trajectory sequence length')
    parser.add_argument('--style_mode', type=str, default='random',
                        choices=['random', 'safe', 'fast', 'balanced'],
                        help='Style weight generation mode (mock only)')
    parser.add_argument('--train_ratio', type=float, default=0.9,
                        help='Train/val split ratio when data_source=generated')
    
    # Model parameters
    parser.add_argument('--model_type', type=str, default='transformer',
                        choices=['transformer', 'unet1d'],
                        help='Model architecture')
    parser.add_argument('--hidden_dim', type=int, default=256,
                        help='Hidden dimension')
    parser.add_argument('--num_layers', type=int, default=8,
                        help='Number of transformer layers')
    parser.add_argument('--num_heads', type=int, default=8,
                        help='Number of attention heads')
    
    # Training parameters
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Learning rate')
    parser.add_argument('--weight_decay', type=float, default=1e-5,
                        help='Weight decay')
    parser.add_argument('--lambda_vel', type=float, default=1.0,
                        help='Loss weight for velocity field (position flow)')
    parser.add_argument('--lambda_acc', type=float, default=1.0,
                        help='Loss weight for acceleration field (velocity flow)')
    parser.add_argument('--lambda_jerk', type=float, default=0.1,
                        help='Loss weight for jerk field (acc flow). Lower if Acc loss dominates and does not drop.')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='Device to use')
    
    # Checkpoint parameters
    parser.add_argument('--save_dir', type=str, default='checkpoints_l2',
                        help='Directory to save checkpoints')
    parser.add_argument('--save_interval', type=int, default=10,
                        help='Save checkpoint every N epochs')
    parser.add_argument('--resume', type=str, default=None,
                        help='Resume from checkpoint')
    
    return parser.parse_args()


def create_datasets_and_loaders(args):
    """Create training and validation datasets and dataloaders."""
    if args.data_source == "generated":
        if not args.data_dir:
            raise ValueError("--data_dir is required when --data_source=generated")
        train_loader, val_loader = create_l2_dataloaders(
            data_source="generated",
            data_dir=args.data_dir,
            batch_size=args.batch_size,
            map_size=args.map_size,
            seq_len=args.seq_len,
            train_ratio=args.train_ratio,
            seed=42,
        )
        return train_loader, val_loader
    
    train_dataset = MockL3Dataset(
        num_samples=args.num_samples,
        map_size=args.map_size,
        seq_len=args.seq_len,
        style_mode=args.style_mode,
        seed=42,
    )
    val_dataset = MockL3Dataset(
        num_samples=args.val_samples,
        map_size=args.map_size,
        seq_len=args.seq_len,
        style_mode=args.style_mode,
        seed=123,
    )
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        drop_last=True,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
    )
    return train_loader, val_loader


def create_model(args):
    """Create L2 model."""
    model = create_l2_safety_cfm(
        model_type=args.model_type,
        state_dim=2,
        max_seq_len=args.seq_len,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        num_heads=args.num_heads,
        cost_map_channels=1,
        cost_map_latent_dim=256,
        cost_map_encoder_type='single_scale',
        use_style_conditioning=True,
        style_dim=3,  # [w_safety, w_energy, w_smooth]
        use_8step_schedule=True,
    )
    return model


def train_epoch(model, train_loader, optimizer, flow_interpolator, loss_fn, device, epoch):
    """Train for one epoch."""
    model.train()
    
    total_loss = 0.0
    total_pos_loss = 0.0
    total_vel_loss = 0.0
    total_acc_loss = 0.0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    for batch_idx, batch in enumerate(pbar):
        # Move to device
        cost_map = batch['cost_map'].to(device)           # [B, 1, H, W]
        start_state = batch['start_state'].to(device)     # [B, 6]
        goal_state = batch['goal_state'].to(device)       # [B, 4]
        style_weights = batch['style_weights'].to(device) # [B, 2]
        positions = batch['positions'].to(device)         # [B, T, 2]
        velocities = batch['velocities'].to(device)       # [B, T, 2]
        accelerations = batch['accelerations'].to(device) # [B, T, 2]
        
        # Concatenate full state [p, v, a]
        full_trajectory = torch.cat([positions, velocities, accelerations], dim=-1)  # [B, T, 6]
        
        # Sample flow time
        B = cost_map.shape[0]
        t = torch.rand(B, 1, device=device)  # [B, 1]
        
        # Interpolate trajectory (flow matching)
        interpolated = flow_interpolator.interpolate_trajectory(
            q_1=full_trajectory[:, :, :2],    # positions [B, T, 2]
            q_dot_1=full_trajectory[:, :, 2:4],     # velocities [B, T, 2]
            q_ddot_1=full_trajectory[:, :, 4:],   # accelerations [B, T, 2]
            t=t.squeeze(-1),
        )
        
        x_t = interpolated['x_t']      # [B, T, 6]
        target_u = interpolated['target']  # [B, T, 6]
        
        # Forward pass
        t_flat = t.squeeze(-1)  # [B]
        predicted_u = model(
            x_t=x_t,
            t=t_flat,
            cost_map=cost_map,
            x_curr=start_state,
            x_goal=goal_state,
            w_style=style_weights,
        )
        
        # Compute loss (FlowMatchingLoss returns 'loss', 'loss_vel', 'loss_acc', 'loss_jerk')
        raw = loss_fn(predicted_u, target_u)
        loss_dict = {
            'total_loss': raw['loss'],
            'position_loss': raw['loss_vel'],
            'velocity_loss': raw['loss_acc'],
            'acceleration_loss': raw['loss_jerk'],
        }
        loss = loss_dict['total_loss']
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # Accumulate losses
        total_loss += loss.item()
        total_pos_loss += loss_dict['position_loss'].item()
        total_vel_loss += loss_dict['velocity_loss'].item()
        total_acc_loss += loss_dict['acceleration_loss'].item()
        
        # Update progress bar
        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'pos': f"{loss_dict['position_loss'].item():.4f}",
            'vel': f"{loss_dict['velocity_loss'].item():.4f}",
            'acc': f"{loss_dict['acceleration_loss'].item():.4f}",
        })
    
    # Average losses
    num_batches = len(train_loader)
    return {
        'total_loss': total_loss / num_batches,
        'position_loss': total_pos_loss / num_batches,
        'velocity_loss': total_vel_loss / num_batches,
        'acceleration_loss': total_acc_loss / num_batches,
    }


@torch.no_grad()
def validate(model, val_loader, flow_interpolator, loss_fn, device):
    """Validate the model."""
    model.eval()
    
    total_loss = 0.0
    total_pos_loss = 0.0
    total_vel_loss = 0.0
    total_acc_loss = 0.0
    
    for batch in tqdm(val_loader, desc="Validation"):
        # Move to device
        cost_map = batch['cost_map'].to(device)
        start_state = batch['start_state'].to(device)
        goal_state = batch['goal_state'].to(device)
        style_weights = batch['style_weights'].to(device)
        positions = batch['positions'].to(device)
        velocities = batch['velocities'].to(device)
        accelerations = batch['accelerations'].to(device)
        
        # Concatenate full state
        full_trajectory = torch.cat([positions, velocities, accelerations], dim=-1)
        
        # Sample flow time
        B = cost_map.shape[0]
        t = torch.rand(B, 1, device=device)
        
        # Interpolate
        interpolated = flow_interpolator.interpolate_trajectory(
            q_1=full_trajectory[:, :, :2],
            q_dot_1=full_trajectory[:, :, 2:4],
            q_ddot_1=full_trajectory[:, :, 4:],
            t=t.squeeze(-1),
        )
        
        x_t = interpolated['x_t']
        target_u = interpolated['target']
        
        # Forward pass
        t_flat = t.squeeze(-1)
        predicted_u = model(
            x_t=x_t,
            t=t_flat,
            cost_map=cost_map,
            x_curr=start_state,
            x_goal=goal_state,
            w_style=style_weights,
        )
        
        # Compute loss
        raw = loss_fn(predicted_u, target_u)
        loss_dict = {
            'total_loss': raw['loss'],
            'position_loss': raw['loss_vel'],
            'velocity_loss': raw['loss_acc'],
            'acceleration_loss': raw['loss_jerk'],
        }
        
        total_loss += loss_dict['total_loss'].item()
        total_pos_loss += loss_dict['position_loss'].item()
        total_vel_loss += loss_dict['velocity_loss'].item()
        total_acc_loss += loss_dict['acceleration_loss'].item()
    
    num_batches = len(val_loader)
    return {
        'total_loss': total_loss / num_batches,
        'position_loss': total_pos_loss / num_batches,
        'velocity_loss': total_vel_loss / num_batches,
        'acceleration_loss': total_acc_loss / num_batches,
    }


def save_checkpoint(model, optimizer, epoch, losses, save_path):
    """Save model checkpoint."""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'losses': losses,
    }
    torch.save(checkpoint, save_path)
    print(f"Saved checkpoint to {save_path}")


def main():
    args = parse_args()
    
    # Create save directory
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("L2 Layer Training with Mock L3 Data")
    print("=" * 80)
    print(f"Device: {args.device}")
    print(f"Data source: {args.data_source}")
    if args.data_source == "mock":
        print(f"Training samples: {args.num_samples}")
        print(f"Validation samples: {args.val_samples}")
        print(f"Style mode: {args.style_mode}")
    else:
        print(f"Data dir: {args.data_dir}")
        print(f"Train ratio: {args.train_ratio}")
    print(f"Model type: {args.model_type}")
    print(f"Epochs: {args.epochs}")
    print(f"Batch size: {args.batch_size}")
    print(f"Loss weights: lambda_vel={args.lambda_vel}, lambda_acc={args.lambda_acc}, lambda_jerk={args.lambda_jerk}")
    print("=" * 80)
    
    # Create datasets and dataloaders
    print("\nCreating datasets and dataloaders...")
    train_loader, val_loader = create_datasets_and_loaders(args)
    
    n_train = len(train_loader.dataset)
    n_batches = len(train_loader)
    print(f"Train samples: {n_train}, batches per epoch: {n_batches}")
    print(f"Validation batches: {len(val_loader)}")
    if args.data_source == "generated" and n_batches < 5:
        print(f"  [Tip] Few batches per epoch ({n_batches}) can slow convergence. Try --batch_size {max(64, n_train // 8)} for more updates.")
    
    # Create model
    print("\nCreating model...")
    model = create_model(args).to(args.device)
    
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {num_params:,}")
    
    # Create optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )
    
    # Create flow interpolator and loss (lambda_jerk < 1 avoids jerk-dominated gradient when data |acc| is large)
    flow_config = FlowMatchingConfig(
        lambda_vel=args.lambda_vel,
        lambda_acc=args.lambda_acc,
        lambda_jerk=args.lambda_jerk,
    )
    flow_interpolator = FlowInterpolator(flow_config)
    loss_fn = FlowMatchingLoss(flow_config)
    
    # Resume from checkpoint if specified
    start_epoch = 0
    if args.resume:
        print(f"\nResuming from checkpoint: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=args.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
    
    # Training loop
    print("\n" + "=" * 80)
    print("Starting training...")
    print("=" * 80)
    
    best_val_loss = float('inf')
    
    for epoch in range(start_epoch, args.epochs):
        print(f"\nEpoch {epoch + 1}/{args.epochs}")
        
        # Train
        train_losses = train_epoch(
            model, train_loader, optimizer, flow_interpolator, loss_fn, args.device, epoch + 1
        )
        
        print(f"Train Loss: {train_losses['total_loss']:.4f} | "
              f"Pos: {train_losses['position_loss']:.4f} | "
              f"Vel: {train_losses['velocity_loss']:.4f} | "
              f"Acc: {train_losses['acceleration_loss']:.4f}")
        
        # Validate
        val_losses = validate(model, val_loader, flow_interpolator, loss_fn, args.device)
        
        print(f"Val Loss:   {val_losses['total_loss']:.4f} | "
              f"Pos: {val_losses['position_loss']:.4f} | "
              f"Vel: {val_losses['velocity_loss']:.4f} | "
              f"Acc: {val_losses['acceleration_loss']:.4f}")
        
        # Save checkpoint
        if (epoch + 1) % args.save_interval == 0:
            save_path = save_dir / f"checkpoint_epoch_{epoch+1}.pt"
            save_checkpoint(model, optimizer, epoch, {'train': train_losses, 'val': val_losses}, save_path)
        
        # Save best model
        if val_losses['total_loss'] < best_val_loss:
            best_val_loss = val_losses['total_loss']
            save_path = save_dir / "best_model.pt"
            save_checkpoint(model, optimizer, epoch, {'train': train_losses, 'val': val_losses}, save_path)
            print(f"âœ“ New best model! Val loss: {best_val_loss:.4f}")
    
    # Save final model
    save_path = save_dir / "final_model.pt"
    save_checkpoint(model, optimizer, args.epochs - 1, {'train': train_losses, 'val': val_losses}, save_path)
    
    print("\n" + "=" * 80)
    print("Training complete!")
    print(f"Best validation loss: {best_val_loss:.4f}")
    print(f"Models saved to: {save_dir}")
    print("=" * 80)


if __name__ == "__main__":
    main()
